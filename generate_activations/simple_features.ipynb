{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('/home3/ebrahim2/beyond-brainscore/analyze_results/figures_code/')\n",
    "from trained_untrained_results_funcs import find_best_layer, loop_through_datasets\n",
    "# include punctuation\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pereira "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "sigma_range = np.round(np.arange(0, 4.8, 0.1),2)\n",
    "# load the positional and word rate information \n",
    "positional_WN = np.load('/data/LLMs/data_processed/pereira/acts/X_positional_WN.npz')['layer1']\n",
    "psg_start = np.where(positional_WN[:,0] == 1)\n",
    "# Add the final index to make the following for loop work\n",
    "psg_start = np.insert(psg_start[0], 168, 627)\n",
    "positional_WN_dict = {}\n",
    "for s in sigma_range:\n",
    "    positional_WN_dict[f'layer_{s}'] = deepcopy(positional_WN)\n",
    "    if s!=0:\n",
    "        for i in range(len(psg_start)-1):\n",
    "                positional_WN_dict[f'layer_{s}'][ psg_start[i] : psg_start[i+1] , 0 : psg_start[i+1] -psg_start[i]] =  gaussian_filter1d(\n",
    "                positional_WN[ psg_start[i] : psg_start[i+1] , 0 : psg_start[i+1] -psg_start[i]], sigma=s) \n",
    "                \n",
    "np.savez('/data/LLMs/data_processed/pereira/acts/X_positional_WN_smooth.npz', **positional_WN_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fedorenko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_len = []\n",
    "with open('/data/LLMs/data_processed/fedorenko/text/sentences_ordered.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        word_len.extend([len(word) for word in words])\n",
    "    \n",
    "word_len = np.expand_dims(word_len, axis=-1)\n",
    "growing_act = np.expand_dims(np.tile([0, 1, 2, 3, 4, 5, 6, 7], 52),axis=-1)\n",
    "\n",
    "# Initialize the matrix\n",
    "rows, cols = 416, 8\n",
    "pos_matrix = np.zeros((rows, cols))\n",
    "\n",
    "# Fill the matrix with the cyclic one-hot encoding\n",
    "for i in range(rows):\n",
    "    pos_matrix[i, i % cols] = 1\n",
    "    \n",
    "sigma_range = np.round(np.arange(0, 4.8, 0.1),2)\n",
    "position_fed = {}\n",
    "wn_position_fed = {}\n",
    "for s in sigma_range:\n",
    "    if s == 0:\n",
    "        print(s)\n",
    "        position_fed[f'layer_{s}'] = np.hstack((pos_matrix, growing_act))\n",
    "        wn_position_fed[f'layer_{s}'] = np.hstack((pos_matrix, growing_act, word_len)) \n",
    "    else:\n",
    "        position_fed[f'layer_{s}'] =  np.hstack((np.array([gaussian_filter1d(row, sigma=s) for row in pos_matrix]),growing_act))\n",
    "        wn_position_fed[f'layer_{s}'] = np.hstack((np.array([gaussian_filter1d(row, sigma=s) for row in pos_matrix]),growing_act, word_len))\n",
    "\n",
    "\n",
    "np.savez('/data/LLMs/data_processed/fedorenko/acts/X_pos.npz', **position_fed)\n",
    "np.savez('/data/LLMs/data_processed/fedorenko/acts/X_wn+pos.npz', **wn_position_fed)\n",
    "#fed_position = np.load('/data/LLMs/data_processed/fedorenko/acts/X_soft+grow.npz')['layer1']\n",
    "#fed_position_word_len = np.hstack((fed_position, np.expand_dims(word_len,axis=-1)))\n",
    "#np.savez('/data/LLMs/data_processed/fedorenko/acts/X_soft+grow.npz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_for_story(len_story, len_pos=30):\n",
    "    return np.clip((np.arange(0,len_pos) * np.ones((len_story,len_pos))).T,\n",
    "                    np.zeros(len_story), np.clip(np.arange(0,len_story),0,len_pos) ).T\n",
    "    \n",
    "def get_pos_for_stories(story_lens, len_pos=30):\n",
    "    return np.concatenate([get_pos_for_story(l, len_pos) for l in story_lens])\n",
    "\n",
    "\n",
    "dataset='blank'\n",
    "blank_data = np.load(f\"/data/LLMs/data_processed/{dataset}/text/story_data_dict.npz\")\n",
    "experiment_txt = []\n",
    "data_labels = []\n",
    "for key, val in blank_data.items():\n",
    "    experiment_txt.extend(val)\n",
    "    data_labels.extend(np.repeat(key, len(val)))\n",
    "\n",
    "story_lens = np.diff(np.append(np.unique(data_labels, return_index=True)[1],1317))\n",
    "blank_simple_acts = {}\n",
    "blank_simple_acts_pos = {}\n",
    "\n",
    "for i in np.arange(3,51):\n",
    "    blank_acts_pos = get_pos_for_stories(story_lens,i) # get the ramping activations used by Antonello \n",
    "    blank_acts_pos = blank_acts_pos[:, 1:] # remove the 0th dimension since it's a constant anyways\n",
    "    blank_acts = np.concatenate((np.array(([len(x.split()) for x in experiment_txt] * 1)).reshape(1,1317).T, blank_acts_pos),axis=1) # stack on a word rate feature\n",
    "    blank_simple_acts[f'layer_{i}'] = blank_acts\n",
    "    blank_simple_acts_pos[f\"layer_{i}\"] = blank_simple_acts_pos\n",
    "    \n",
    "np.savez('/data/LLMs/data_processed/blank/acts/X_pos-WN', **blank_simple_acts)\n",
    "np.savez('/data/LLMs/data_processed/blank/acts/X_pos', **blank_simple_acts_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
