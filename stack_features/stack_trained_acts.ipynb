{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home2/ebrahim/beyond-brainscore/analyze_results/figures_code')\n",
    "from trained_untrained_results_funcs import find_best_layer, loop_through_datasets\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_combinations(input_dict, exclude_pairs=None, merge_sizes=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stacks every combination of numpy arrays in the input dictionary and creates two dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dict (dict): Dictionary where keys are strings and values are numpy arrays.\n",
    "    exclude_pairs (list of tuples): List of pairs of keys that should not be stacked together.\n",
    "    merge_sizes (list of tuples): List of pairs of keys whose sizes should be combined into a single value.\n",
    "    \n",
    "    Returns:\n",
    "    tuple:\n",
    "        - A dictionary with keys as combinations of the original keys and values as stacked arrays.\n",
    "        - A dictionary with the same keys, but values are lists of the sizes of the concatenated arrays,\n",
    "          with specified pairs having their sizes combined.\n",
    "    \"\"\"\n",
    "    \n",
    "    if exclude_pairs is None:\n",
    "        exclude_pairs = []\n",
    "    if merge_sizes is None:\n",
    "        merge_sizes = []\n",
    "\n",
    "    # Normalize the exclude_pairs and merge_sizes to ensure order doesn't matter\n",
    "    exclude_pairs = {tuple(sorted(pair)) for pair in exclude_pairs}\n",
    "    merge_sizes = {tuple(sorted(pair)) for pair in merge_sizes}\n",
    "    \n",
    "    output_dict = {}\n",
    "    size_dict = {}\n",
    "    keys = list(input_dict.keys())\n",
    "    \n",
    "    # Generate all combinations of keys (at least 2 keys in a combination)\n",
    "    for r in range(1, len(keys) + 1):\n",
    "        for combination in itertools.combinations(keys, r):\n",
    "            # Check if any excluded pair exists in the combination\n",
    "            skip_combination = False\n",
    "            for pair in itertools.combinations(combination, 2):\n",
    "                if tuple(sorted(pair)) in exclude_pairs:\n",
    "                    skip_combination = True\n",
    "                    break\n",
    "            \n",
    "            if skip_combination:\n",
    "                continue\n",
    "            \n",
    "            # Stack the corresponding arrays\n",
    "            arrays = [input_dict[key] for key in combination]\n",
    "            stacked_array = np.hstack(arrays)\n",
    "            \n",
    "            # Create a new key by joining the original keys with '+'\n",
    "            new_key = '+'.join(combination)\n",
    "            output_dict[new_key] = stacked_array\n",
    "            \n",
    "            # Record the sizes of the concatenated arrays\n",
    "            sizes = []\n",
    "            merged_keys = set()\n",
    "            \n",
    "            for i, key1 in enumerate(combination):\n",
    "                if key1 in merged_keys:\n",
    "                    continue\n",
    "                size = input_dict[key1].shape[1]\n",
    "                \n",
    "                # Check if this key should be merged with another\n",
    "                for key2 in combination[i+1:]:\n",
    "                    if tuple(sorted((key1, key2))) in merge_sizes:\n",
    "                        size += input_dict[key2].shape[1]\n",
    "                        merged_keys.add(key2)\n",
    "                \n",
    "                merged_keys.add(key1)\n",
    "                sizes.append(size)\n",
    "            \n",
    "            size_dict[new_key] = sizes\n",
    "    \n",
    "    return output_dict, size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gpt2xl_layer_dict = np.load('/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/best_layer_sigma_info/best_gpt2xl_layer.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(25)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gpt2xl_layer_dict['pereira_384_pearson_r_contig_noL2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pereira_pos_WN_model = np.load(\"/data/LLMs/data_processed/pereira/acts/X_positional_WN.npz\")['layer1']\n",
    "pereira_pos_model = np.load(\"/data/LLMs/data_processed/pereira/acts/X_positional_simple.npz\")['layer1']\n",
    "pereira_WN_model = np.load(\"/data/LLMs/data_processed/pereira/acts/X_word-num.npz\")['layer1']\n",
    "pereira_glove_sp = np.load(\"/data/LLMs/data_processed/pereira/acts/X_glove-sp.npz\")['layer_0']\n",
    "pereira_all_simple_models = {'pos': pereira_pos_model, 'WN': pereira_WN_model, 'glove': pereira_glove_sp}\n",
    "\n",
    "fedorenko_WP_model = np.load(\"/data/LLMs/data_processed/fedorenko/acts/X_pos.npz\")['layer_4.6']\n",
    "fed_glove_sp = np.load(\"/data/LLMs/data_processed/fedorenko/acts/X_glove-sp.npz\")['layer_0']\n",
    "fedorenko_all_simple_models = {'WP': fedorenko_WP_model, 'glove': fed_glove_sp}\n",
    "\n",
    "blank_pos_model = np.load(\"/data/LLMs/data_processed/blank/acts/X_POS.npz\")['layer1'] \n",
    "blank_WN_model = np.load(\"/data/LLMs/data_processed/blank/acts/X_WN.npz\")['layer1'] \n",
    "blank_pos_WN_model = np.load(\"/data/LLMs/data_processed/blank/acts/X_pos-WN.npz\")['layer_12']\n",
    "blank_glove_sp = np.load(\"/data/LLMs/data_processed/blank/acts/X_glove-sp.npz\")['layer_0']\n",
    "blank_all_simple_models = {'pos': blank_pos_model, 'WN': blank_WN_model, 'glove': blank_glove_sp}\n",
    "\n",
    "all_simple_models = {'pereira': pereira_all_simple_models, 'fedorenko': fedorenko_all_simple_models, 'blank': blank_all_simple_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, fe, exp, subjects, network in loop_through_datasets(['pereira', 'fedorenko', 'blank'], ['', '-mp', '-sp']):\n",
    "\n",
    "    gpt2xl_acts = np.load(f\"/data/LLMs/data_processed/{dataset}/acts/X_gpt2-xl{fe}.npz\")\n",
    "    \n",
    "    gpt2xl_best_layer = best_gpt2xl_layer_dict[f'{dataset}{exp}_out_of_sample_r2_contig']\n",
    "\n",
    "    all_simple_models[dataset]['gpt2xl'] = gpt2xl_acts[f\"layer_{gpt2xl_best_layer}\"]\n",
    "    \n",
    "    \n",
    "    if dataset == 'pereira':\n",
    "        \n",
    "        #syntax_acts = np.load(f'/data/LLMs/data_processed/pereira/acts/X_gpt2xl-syntax{fe}.npz')\n",
    "        #gpt2xl_best_layer_pereira = best_gpt2xl_layer_dict[f'pereira{exp}_out_of_sample_r2_contig']\n",
    "        \n",
    "        pass\n",
    "            \n",
    "        #all_simple_models[dataset]['syntax'] = X_synt_dict[f\"{fe}_{exp}\"]\n",
    "        #all_simple_models[dataset]['syntax-v2'] = syntax_acts[f\"layer_{gpt2xl_best_layer_pereira}\"]\n",
    "        \n",
    "    all_simple_models_dict, feature_list_dict = stack_combinations(all_simple_models[dataset], merge_sizes=[('pos', 'WN')])\n",
    "    \n",
    "    np.savez(f'/data/LLMs/data_processed/{dataset}/acts/X_trained-var-par{exp}{fe}', **all_simple_models_dict)\n",
    "    np.savez(f'/data/LLMs/data_processed/{dataset}/acts/f-list_trained-var-par{exp}{fe}', **feature_list_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'WN', 'glove', 'gpt2xl', 'pos+WN', 'pos+glove', 'pos+gpt2xl', 'WN+glove', 'WN+gpt2xl', 'glove+gpt2xl', 'pos+WN+glove', 'pos+WN+gpt2xl', 'pos+glove+gpt2xl', 'WN+glove+gpt2xl', 'pos+WN+glove+gpt2xl'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_simple_models_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
