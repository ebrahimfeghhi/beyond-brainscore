{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home2/ebrahim/beyond-brainscore/analyze_results/figures_code')\n",
    "from trained_untrained_results_funcs import find_best_layer, loop_through_datasets\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_combinations(input_dict, exclude_pairs=None, merge_sizes=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stacks every combination of numpy arrays in the input dictionary and creates two dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dict (dict): Dictionary where keys are strings and values are numpy arrays.\n",
    "    exclude_pairs (list of tuples): List of pairs of keys that should not be stacked together.\n",
    "    merge_sizes (list of tuples): List of pairs of keys whose sizes should be combined into a single value.\n",
    "    \n",
    "    Returns:\n",
    "    tuple:\n",
    "        - A dictionary with keys as combinations of the original keys and values as stacked arrays.\n",
    "        - A dictionary with the same keys, but values are lists of the sizes of the concatenated arrays,\n",
    "          with specified pairs having their sizes combined.\n",
    "    \"\"\"\n",
    "    \n",
    "    if exclude_pairs is None:\n",
    "        exclude_pairs = []\n",
    "    if merge_sizes is None:\n",
    "        merge_sizes = []\n",
    "\n",
    "    # Normalize the exclude_pairs and merge_sizes to ensure order doesn't matter\n",
    "    exclude_pairs = {tuple(sorted(pair)) for pair in exclude_pairs}\n",
    "    merge_sizes = {tuple(sorted(pair)) for pair in merge_sizes}\n",
    "    \n",
    "    output_dict = {}\n",
    "    size_dict = {}\n",
    "    keys = list(input_dict.keys())\n",
    "    \n",
    "    # Generate all combinations of keys (at least 2 keys in a combination)\n",
    "    for r in range(1, len(keys) + 1):\n",
    "        for combination in itertools.combinations(keys, r):\n",
    "            # Check if any excluded pair exists in the combination\n",
    "            skip_combination = False\n",
    "            for pair in itertools.combinations(combination, 2):\n",
    "                if tuple(sorted(pair)) in exclude_pairs:\n",
    "                    skip_combination = True\n",
    "                    break\n",
    "            \n",
    "            if skip_combination:\n",
    "                continue\n",
    "            \n",
    "            # Stack the corresponding arrays\n",
    "            arrays = [input_dict[key] for key in combination]\n",
    "            stacked_array = np.hstack(arrays)\n",
    "            \n",
    "            # Create a new key by joining the original keys with '+'\n",
    "            new_key = '+'.join(combination)\n",
    "            output_dict[new_key] = stacked_array\n",
    "            \n",
    "            # Record the sizes of the concatenated arrays\n",
    "            sizes = []\n",
    "            merged_keys = set()\n",
    "            \n",
    "            for i, key1 in enumerate(combination):\n",
    "                if key1 in merged_keys:\n",
    "                    continue\n",
    "                size = input_dict[key1].shape[1]\n",
    "                \n",
    "                # Check if this key should be merged with another\n",
    "                for key2 in combination[i+1:]:\n",
    "                    if tuple(sorted((key1, key2))) in merge_sizes:\n",
    "                        size += input_dict[key2].shape[1]\n",
    "                        merged_keys.add(key2)\n",
    "                \n",
    "                merged_keys.add(key1)\n",
    "                sizes.append(size)\n",
    "            \n",
    "            size_dict[new_key] = sizes\n",
    "    \n",
    "    return output_dict, size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gpt2xl_layer_dict = np.load('/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/best_layer_sigma_info/best_gpt2xl_layer.npz')\n",
    "best_pereira_layers = np.load('/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/best_layer_sigma_info/best_layer_other_pereira.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pereira_WN_model = np.load(\"/data/LLMs/data_processed/pereira/acts/X_word-num.npz\")['layer1']\n",
    "pereira_glove_sp = np.load(\"/data/LLMs/data_processed/pereira/acts/X_glove-sp.npz\")['layer_0']\n",
    "best_pos = {'384': 3.4, '243': 2.9}\n",
    "pereira_all_simple_models = {'WN': pereira_WN_model, 'glove': pereira_glove_sp}\n",
    "\n",
    "fedorenko_WP_model = np.load(\"/data/LLMs/data_processed/fedorenko/acts/X_pos.npz\")['layer_4.7']\n",
    "#fed_glove_sp = np.load(\"/data/LLMs/data_processed/fedorenko/acts/X_glove-sp.npz\")['layer_0']\n",
    "fedorenko_all_simple_models = {'WP': fedorenko_WP_model}\n",
    "\n",
    "blank_pos_model = np.load(\"/data/LLMs/data_processed/blank/acts/X_pos.npz\")['layer_11'] \n",
    "blank_WN_model = np.load(\"/data/LLMs/data_processed/blank/acts/X_WN.npz\")['layer1'] \n",
    "#blank_glove_sp = np.load(\"/data/LLMs/data_processed/blank/acts/X_glove-sp.npz\")['layer_0']\n",
    "blank_all_simple_models = {'pos': blank_pos_model, 'WN': blank_WN_model}\n",
    "\n",
    "all_simple_models = {'pereira': pereira_all_simple_models, 'fedorenko': fedorenko_all_simple_models, 'blank': blank_all_simple_models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "21\n",
      "16\n",
      "21\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "for dataset, fe, exp, subjects, network in loop_through_datasets(['pereira', 'fedorenko', 'blank'], ['', '-mp', '-sp']):\n",
    "\n",
    "    gpt2xl_acts = np.load(f\"/data/LLMs/data_processed/{dataset}/acts/X_gpt2-xl{fe}.npz\")\n",
    "    \n",
    "    gpt2xl_best_layer = best_gpt2xl_layer_dict[f'{dataset}{exp}_out_of_sample_r2_contig{fe}']\n",
    "\n",
    "    all_simple_models[dataset]['gpt2xl'] = gpt2xl_acts[f\"layer_{gpt2xl_best_layer}\"]\n",
    "    \n",
    "    \n",
    "    if dataset == 'pereira':\n",
    "        exp = exp.strip('_')\n",
    "        position_pereira = np.load(\"/data/LLMs/data_processed/pereira/acts/X_position.npz\")[f'layer_{best_pos[exp]}']\n",
    "        all_simple_models[dataset]['pos'] = position_pereira\n",
    "        \n",
    "        gpt2xl_acts_decontext = np.load(f\"/data/LLMs/data_processed/{dataset}/acts/X_gpt2-xl-decontext{fe}.npz\")\n",
    "        \n",
    "        gpt2_decontext_best_layer = best_gpt2xl_layer_dict[f'{dataset}_{exp}_pearson_r_contig{fe}']\n",
    "        print(gpt2_decontext_best_layer)\n",
    "        \n",
    "        gpt2_decontext_p = {'layer1': np.hstack((gpt2xl_acts_decontext[f\"layer_{gpt2_decontext_best_layer}\"], position_pereira))}\n",
    "        \n",
    "        np.savez(f'/data/LLMs/data_processed/{dataset}/acts/X_gpt2xl-decontext_position_{exp}{fe}_pearsonr', **gpt2_decontext_p)\n",
    "        \n",
    "        \n",
    "    all_simple_models_dict, feature_list_dict = stack_combinations(all_simple_models[dataset], merge_sizes=[('pos', 'WN')])\n",
    "    \n",
    "    np.savez(f'/data/LLMs/data_processed/{dataset}/acts/X_trained-var-par{exp}{fe}', **all_simple_models_dict)\n",
    "    np.savez(f'/data/LLMs/data_processed/{dataset}/acts/f-list_trained-var-par{exp}{fe}', **feature_list_dict)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile '/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/best_layer_sigma_info/best_layer_other_pereira.npz' with keys: PWR_pearson_r_243, GPT2XL-decontext_pearson_r_243_, PWR_out_of_sample_r2_243, GPT2XL-decontext_out_of_sample_r2_243_, PWR_pearson_r_384..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pereira_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_xl_decontext_best_layers = {'-lt': 24, '-mp': }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
