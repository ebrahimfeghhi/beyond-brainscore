{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trained_untrained_results_funcs import loop_through_datasets, load_mean_sem_perf, custom_add_2d\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import false_discovery_control\n",
    "\n",
    "def compute_voxel_pvalues(voxel_performance, null_distribution):\n",
    "    \"\"\"\n",
    "    Compute one-sided p-values for voxel performance against a null distribution and apply FDR correction.\n",
    "    \n",
    "    Parameters:\n",
    "    - voxel_performance: 1D array of shape (num_voxels/elecs/fROIs,) containing the difference in performance values between m1 and m2.\n",
    "    - null_distribution: 2D array of shape (1000, num_voxels/elecs/fROIs) representing the null distribution for each voxel.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    - p_values: 1D array of uncorrected p-values for each voxel. The pvalue indicates the chance that the difference between m1 and m2 is due\n",
    "    to chance. The way this is computed is we fit N=1000 gaussian regressions, and use these to create a null distribution of R2 values. \n",
    "    Then, we compute the fraction of null R2 values that are greater than or equal to the difference in R2 between m1 and m2 to get the p-value.\n",
    "    \n",
    "    - fdr_corrected_p_values: 1D array of FDR-corrected p-values for each voxel.\n",
    "    \"\"\"\n",
    "    num_voxels = voxel_performance.shape[0]\n",
    "    p_values = np.zeros(num_voxels)\n",
    "\n",
    "    # Compute p-values for each voxel\n",
    "    for i in range(num_voxels):\n",
    "        null_dist = null_distribution[:, i]\n",
    "\n",
    "        p_values[i] = np.nanmean(null_dist >= voxel_performance[i])  # One-sided test\n",
    "        \n",
    "    fdr_corrected_p_values  = false_discovery_control(p_values)\n",
    "    \n",
    "        # Compute and print fractions under 0.05\n",
    "    frac_uncorrected = np.mean(p_values < 0.05) * 100\n",
    "    frac_fdr_corrected = np.mean(fdr_corrected_p_values < 0.05) * 100 \n",
    "\n",
    "    print(f\"Fraction of uncorrected p-values < 0.05: {frac_uncorrected:.2f}\")\n",
    "    print(f\"Fraction of FDR-corrected p-values < 0.05: {frac_fdr_corrected:.2f}\")\n",
    "\n",
    "\n",
    "    return p_values, fdr_corrected_p_values\n",
    "\n",
    "           \n",
    "def process_nan_indices(arr1, arr2, large_negative_value=-1e9):\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes two arrays to handle NaN values as described:\n",
    "    1. If both arrays have NaN at the same index, remove that index.\n",
    "    2. If one array has a NaN and the other doesn't, replace the NaN with a large negative value.\n",
    "\n",
    "    Parameters:\n",
    "    - arr1 (numpy.ndarray): The first array.\n",
    "    - arr2 (numpy.ndarray): The second array.\n",
    "    - large_negative_value (float): The value to replace NaN with when the other array doesn't have a NaN.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray, numpy.ndarray: The processed arrays.\n",
    "    \"\"\"\n",
    "    arr1, arr2 = np.asarray(arr1), np.asarray(arr2)\n",
    "    \n",
    "    # Ensure both arrays are of the same shape\n",
    "    if arr1.shape != arr2.shape:\n",
    "        raise ValueError(\"Arrays must have the same shape.\")\n",
    "\n",
    "    # Identify indices where both are NaN\n",
    "    both_nan = np.isnan(arr1) & np.isnan(arr2)\n",
    "\n",
    "    # Filter out indices where both are NaN\n",
    "    arr1, arr2 = arr1[~both_nan], arr2[~both_nan]\n",
    "\n",
    "    # Replace remaining NaNs with the large negative value\n",
    "    arr1 = np.where(np.isnan(arr1), large_negative_value, arr1)\n",
    "    arr2 = np.where(np.isnan(arr2), large_negative_value, arr2)\n",
    "\n",
    "    return arr1, arr2\n",
    "\n",
    "def compute_stats_results(llm_model, simple_model, figure_folder, non_nan_indices_dict, subjects_arr_pereira, \n",
    "                          lang_indices, exp=['384', '243'], dataset_arr=['pereira', 'blank', 'fedorenko'], \n",
    "                          llm_greater=True):\n",
    "\n",
    "    perf = 'out_of_sample_r2'\n",
    "\n",
    "    for dataset in dataset_arr:\n",
    "    \n",
    "        for fe in ['', '-mp', '-sp']:\n",
    "        \n",
    "            all_gauss = []\n",
    "            \n",
    "            llm = np.load(f'/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/figures_data/{figure_folder}/{llm_model}_{dataset}.npz')[fe]\n",
    "            simple = np.load(f'/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/figures_data/{figure_folder}/{simple_model}_{dataset}.npz')['']\n",
    "            \n",
    "            \n",
    "            if llm_greater:\n",
    "                perf_diff = llm-simple\n",
    "                \n",
    "            else:\n",
    "                perf_diff = simple-llm\n",
    "            \n",
    "            for i in range(1000):\n",
    "                \n",
    "                if dataset == 'pereira':\n",
    "                    \n",
    "                    gauss_perf_combined = np.full(subjects_arr_pereira.shape[0], fill_value=np.nan)\n",
    "                    \n",
    "                    for e in exp:\n",
    "                    \n",
    "                        gauss_perf = np.load(f'/data/LLMs/brainscore/results_{dataset}/stats/{dataset}_gaussian-stats_layer_{i}_1_{e}.npz')[perf]\n",
    "                        \n",
    "                        gauss_perf_combined[non_nan_indices_dict[e]] = custom_add_2d(gauss_perf_combined[non_nan_indices_dict[e]],  \n",
    "                                                                                        gauss_perf)\n",
    "                        \n",
    "                    gauss_perf_combined = gauss_perf_combined[lang_indices]\n",
    "                        \n",
    "                else:\n",
    "                    \n",
    "                    gauss_perf_combined = np.load(f'/data/LLMs/brainscore/results_{dataset}/stats/{dataset}_gaussian-stats_layer_{i}_1.npz')[perf]\n",
    "                    \n",
    "                \n",
    "                all_gauss.append(gauss_perf_combined)\n",
    "                \n",
    "            \n",
    "            all_gauss_np = np.stack(all_gauss)\n",
    "            \n",
    "            # the gaussian combined has the nans from both the fact that 384 has constant voxels \n",
    "            # and when combing the results across experiments there is a small number of nans\n",
    "            # so we use it to remove nans before stats testing. \n",
    "            nan_mask = ~np.isnan(gauss_perf_combined) \n",
    "            perf_diff = perf_diff[nan_mask]\n",
    "            all_gauss_np = all_gauss_np[:, nan_mask]\n",
    "            \n",
    "            print(dataset, fe)\n",
    "            _, _ = compute_voxel_pvalues(perf_diff, all_gauss_np)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,)\n"
     ]
    }
   ],
   "source": [
    "exp = ['384', '243']\n",
    "data_processed_folder_pereira = '/data/LLMs/data_processed/pereira/dataset/'\n",
    "\n",
    "br_labels_dict = {}\n",
    "num_vox_dict = {}\n",
    "subjects_dict = {}\n",
    "for e in exp:\n",
    "\n",
    "    bre = np.load(f'{data_processed_folder_pereira}/networks_{e}.npy', allow_pickle=True)\n",
    "    br_labels_dict[e] = bre\n",
    "    num_vox_dict[e] = bre.shape[0]\n",
    "    subjects_dict[e] = np.load(f\"{data_processed_folder_pereira}/subjects_{e}.npy\", allow_pickle=True)\n",
    "    \n",
    "lang_indices_dict = {}\n",
    "lang_indices_384 = np.argwhere(br_labels_dict['384'] == 'language').squeeze()\n",
    "lang_indices_243 = np.argwhere(br_labels_dict['243'] == 'language').squeeze()\n",
    "lang_indices_dict['384'] = lang_indices_384\n",
    "lang_indices_dict['243'] = lang_indices_243\n",
    "\n",
    "subjects_arr_pereira = np.load(f\"{data_processed_folder_pereira}/subjects_complete.npy\", allow_pickle=True)\n",
    "networks_arr_pereira = np.load(f\"{data_processed_folder_pereira}/network_complete.npy\", allow_pickle=True)\n",
    "non_nan_indices_243 = np.load(f\"{data_processed_folder_pereira}/non_nan_indices_243.npy\") # voxels which are in 243\n",
    "non_nan_indices_384 = np.load(f\"{data_processed_folder_pereira}/non_nan_indices_384.npy\") # voxels which are in 384\n",
    "non_nan_indices_dict = {'384': non_nan_indices_384, '243': non_nan_indices_243}\n",
    "lang_indices = np.argwhere(networks_arr_pereira=='language').squeeze()\n",
    "\n",
    "y_384 = np.load(f\"{data_processed_folder_pereira}y_pereira_384.npy\")\n",
    "\n",
    "constant_columns_384 = np.all(y_384 == y_384[0, :], axis=0)\n",
    "\n",
    "# Extract and print constant columns\n",
    "constant_columns_indices_384 = np.where(constant_columns_384)[0]\n",
    "print(constant_columns_indices_384.shape) # There will be 23 nan indices just due to constant activation voxels in 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stats_results('gpt2xl_combined', 'simple_combined', 'figure2', non_nan_indices_dict=non_nan_indices_dict, \n",
    "                      subjects_arr_pereira=subjects_arr_pereira, \n",
    "                      lang_indices=lang_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_stats_results('gpt2xl_combined', 'simple_combined', 'figure2', non_nan_indices_dict=non_nan_indices_dict, \n",
    "                      subjects_arr_pereira=subjects_arr_pereira, \n",
    "                      lang_indices=lang_indices, llm_greater=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pereira \n",
      "Fraction of uncorrected p-values < 0.05: 47.09\n",
      "Fraction of FDR-corrected p-values < 0.05: 24.76\n",
      "pereira -mp\n",
      "Fraction of uncorrected p-values < 0.05: 45.56\n",
      "Fraction of FDR-corrected p-values < 0.05: 20.84\n",
      "pereira -sp\n",
      "Fraction of uncorrected p-values < 0.05: 51.06\n",
      "Fraction of FDR-corrected p-values < 0.05: 28.93\n",
      "blank \n",
      "Fraction of uncorrected p-values < 0.05: 23.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 5.00\n",
      "blank -mp\n",
      "Fraction of uncorrected p-values < 0.05: 26.67\n",
      "Fraction of FDR-corrected p-values < 0.05: 6.67\n",
      "blank -sp\n",
      "Fraction of uncorrected p-values < 0.05: 25.00\n",
      "Fraction of FDR-corrected p-values < 0.05: 10.00\n",
      "fedorenko \n",
      "Fraction of uncorrected p-values < 0.05: 41.24\n",
      "Fraction of FDR-corrected p-values < 0.05: 24.74\n",
      "fedorenko -mp\n",
      "Fraction of uncorrected p-values < 0.05: 40.21\n",
      "Fraction of FDR-corrected p-values < 0.05: 18.56\n",
      "fedorenko -sp\n",
      "Fraction of uncorrected p-values < 0.05: 49.48\n",
      "Fraction of FDR-corrected p-values < 0.05: 22.68\n"
     ]
    }
   ],
   "source": [
    "compute_stats_results('gpt2xl_combined', 'simple_combined', 'figure4', non_nan_indices_dict=non_nan_indices_dict, \n",
    "                      subjects_arr_pereira=subjects_arr_pereira, \n",
    "                      lang_indices=lang_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pereira \n",
      "Fraction of uncorrected p-values < 0.05: 32.45\n",
      "Fraction of FDR-corrected p-values < 0.05: 10.64\n",
      "pereira -mp\n",
      "Fraction of uncorrected p-values < 0.05: 33.36\n",
      "Fraction of FDR-corrected p-values < 0.05: 14.95\n",
      "pereira -sp\n",
      "Fraction of uncorrected p-values < 0.05: 27.66\n",
      "Fraction of FDR-corrected p-values < 0.05: 4.67\n",
      "blank \n",
      "Fraction of uncorrected p-values < 0.05: 65.00\n",
      "Fraction of FDR-corrected p-values < 0.05: 55.00\n",
      "blank -mp\n",
      "Fraction of uncorrected p-values < 0.05: 60.00\n",
      "Fraction of FDR-corrected p-values < 0.05: 56.67\n",
      "blank -sp\n",
      "Fraction of uncorrected p-values < 0.05: 58.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 51.67\n",
      "fedorenko \n",
      "Fraction of uncorrected p-values < 0.05: 46.39\n",
      "Fraction of FDR-corrected p-values < 0.05: 34.02\n",
      "fedorenko -mp\n",
      "Fraction of uncorrected p-values < 0.05: 44.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 17.53\n",
      "fedorenko -sp\n",
      "Fraction of uncorrected p-values < 0.05: 34.02\n",
      "Fraction of FDR-corrected p-values < 0.05: 2.06\n"
     ]
    }
   ],
   "source": [
    "compute_stats_results('gpt2xl_combined', 'simple_combined', 'figure4', non_nan_indices_dict=non_nan_indices_dict, \n",
    "                      subjects_arr_pereira=subjects_arr_pereira, \n",
    "                      lang_indices=lang_indices, llm_greater=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pereira \n",
      "Fraction of uncorrected p-values < 0.05: 11.26\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.10\n",
      "pereira -mp\n",
      "Fraction of uncorrected p-values < 0.05: 19.11\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.56\n",
      "pereira -sp\n",
      "Fraction of uncorrected p-values < 0.05: 17.67\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.40\n",
      "blank \n",
      "Fraction of uncorrected p-values < 0.05: 31.67\n",
      "Fraction of FDR-corrected p-values < 0.05: 6.67\n",
      "blank -mp\n",
      "Fraction of uncorrected p-values < 0.05: 31.67\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.00\n",
      "blank -sp\n",
      "Fraction of uncorrected p-values < 0.05: 28.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.00\n",
      "fedorenko \n",
      "Fraction of uncorrected p-values < 0.05: 14.43\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.00\n",
      "fedorenko -mp\n",
      "Fraction of uncorrected p-values < 0.05: 20.62\n",
      "Fraction of FDR-corrected p-values < 0.05: 4.12\n",
      "fedorenko -sp\n",
      "Fraction of uncorrected p-values < 0.05: 19.59\n",
      "Fraction of FDR-corrected p-values < 0.05: 4.12\n"
     ]
    }
   ],
   "source": [
    "compute_stats_results('gpt2xl_combined', 'simple_combined', 'figure5', non_nan_indices_dict=non_nan_indices_dict, \n",
    "                      subjects_arr_pereira=subjects_arr_pereira, \n",
    "                      lang_indices=lang_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pereira \n",
      "Fraction of uncorrected p-values < 0.05: 62.08\n",
      "Fraction of FDR-corrected p-values < 0.05: 43.02\n",
      "pereira -mp\n",
      "Fraction of uncorrected p-values < 0.05: 48.83\n",
      "Fraction of FDR-corrected p-values < 0.05: 22.47\n",
      "pereira -sp\n",
      "Fraction of uncorrected p-values < 0.05: 47.36\n",
      "Fraction of FDR-corrected p-values < 0.05: 2.35\n",
      "blank \n",
      "Fraction of uncorrected p-values < 0.05: 58.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 55.00\n",
      "blank -mp\n",
      "Fraction of uncorrected p-values < 0.05: 58.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 53.33\n",
      "blank -sp\n",
      "Fraction of uncorrected p-values < 0.05: 63.33\n",
      "Fraction of FDR-corrected p-values < 0.05: 55.00\n",
      "fedorenko \n",
      "Fraction of uncorrected p-values < 0.05: 65.98\n",
      "Fraction of FDR-corrected p-values < 0.05: 63.92\n",
      "fedorenko -mp\n",
      "Fraction of uncorrected p-values < 0.05: 64.95\n",
      "Fraction of FDR-corrected p-values < 0.05: 48.45\n",
      "fedorenko -sp\n",
      "Fraction of uncorrected p-values < 0.05: 62.89\n",
      "Fraction of FDR-corrected p-values < 0.05: 56.70\n"
     ]
    }
   ],
   "source": [
    "compute_stats_results('gpt2xl_combined', 'simple_combined', 'figure5', non_nan_indices_dict=non_nan_indices_dict, \n",
    "                      subjects_arr_pereira=subjects_arr_pereira, \n",
    "                      lang_indices=lang_indices, llm_greater=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
