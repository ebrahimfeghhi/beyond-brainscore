{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from trained_untrained_results_funcs import loop_through_datasets, load_mean_sem_perf, custom_add_2d\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import false_discovery_control\n",
    "\n",
    "def compute_voxel_pvalues(voxel_performance, null_distribution):\n",
    "    \"\"\"\n",
    "    Compute one-sided p-values for voxel performance against a null distribution and apply FDR correction.\n",
    "    \n",
    "    Parameters:\n",
    "    - voxel_performance: 1D array of shape (num_voxels,) containing the voxel performance values.\n",
    "    - null_distribution: 2D array of shape (1000, num_voxels) representing the null distribution for each voxel.\n",
    "    \n",
    "    Returns:\n",
    "    - p_values: 1D array of uncorrected p-values for each voxel.\n",
    "    - fdr_corrected_p_values: 1D array of FDR-corrected p-values for each voxel.\n",
    "    \"\"\"\n",
    "    num_voxels = voxel_performance.shape[0]\n",
    "    p_values = np.zeros(num_voxels)\n",
    "\n",
    "    # Compute p-values for each voxel\n",
    "    for i in range(num_voxels):\n",
    "        null_dist = null_distribution[:, i]\n",
    "        p_values[i] = np.nanmean(null_dist >= voxel_performance[i])  # One-sided test\n",
    "        \n",
    "    fdr_corrected_p_values  = false_discovery_control(p_values)\n",
    "    \n",
    "        # Compute and print fractions under 0.05\n",
    "    frac_uncorrected = np.mean(p_values < 0.05) * 100\n",
    "    frac_fdr_corrected = np.mean(fdr_corrected_p_values < 0.05) * 100 \n",
    "\n",
    "    print(f\"Fraction of uncorrected p-values < 0.05: {frac_uncorrected:.3f}\")\n",
    "    print(f\"Fraction of FDR-corrected p-values < 0.05: {frac_fdr_corrected:.3f}\")\n",
    "\n",
    "\n",
    "    return p_values, fdr_corrected_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ['384', '243']\n",
    "data_processed_folder_pereira = '/data/LLMs/data_processed/pereira/dataset/'\n",
    "\n",
    "br_labels_dict = {}\n",
    "num_vox_dict = {}\n",
    "subjects_dict = {}\n",
    "for e in exp:\n",
    "\n",
    "    bre = np.load(f'{data_processed_folder_pereira}/networks_{e}.npy', allow_pickle=True)\n",
    "    br_labels_dict[e] = bre\n",
    "    num_vox_dict[e] = bre.shape[0]\n",
    "    subjects_dict[e] = np.load(f\"{data_processed_folder_pereira}/subjects_{e}.npy\", allow_pickle=True)\n",
    "    \n",
    "lang_indices_dict = {}\n",
    "lang_indices_384 = np.argwhere(br_labels_dict['384'] == 'language').squeeze()\n",
    "lang_indices_243 = np.argwhere(br_labels_dict['243'] == 'language').squeeze()\n",
    "lang_indices_dict['384'] = lang_indices_384\n",
    "lang_indices_dict['243'] = lang_indices_243\n",
    "\n",
    "subjects_arr_pereira = np.load(f\"{data_processed_folder_pereira}/subjects_complete.npy\", allow_pickle=True)\n",
    "networks_arr_pereira = np.load(f\"{data_processed_folder_pereira}/network_complete.npy\", allow_pickle=True)\n",
    "non_nan_indices_243 = np.load(f\"{data_processed_folder_pereira}/non_nan_indices_243.npy\") # voxels which are in 243\n",
    "non_nan_indices_384 = np.load(f\"{data_processed_folder_pereira}/non_nan_indices_384.npy\") # voxels which are in 384\n",
    "non_nan_indices_dict = {'384': non_nan_indices_384, '243': non_nan_indices_243}\n",
    "lang_indices = np.argwhere(networks_arr_pereira=='language').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pereira \n",
      "Fraction of uncorrected p-values < 0.05: 11.599\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.487\n",
      "pereira -mp\n",
      "Fraction of uncorrected p-values < 0.05: 19.420\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.944\n",
      "pereira -sp\n",
      "Fraction of uncorrected p-values < 0.05: 17.989\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.782\n",
      "fedorenko \n",
      "Fraction of uncorrected p-values < 0.05: 14.433\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.000\n",
      "fedorenko -mp\n",
      "Fraction of uncorrected p-values < 0.05: 20.619\n",
      "Fraction of FDR-corrected p-values < 0.05: 4.124\n",
      "fedorenko -sp\n",
      "Fraction of uncorrected p-values < 0.05: 19.588\n",
      "Fraction of FDR-corrected p-values < 0.05: 4.124\n",
      "blank \n",
      "Fraction of uncorrected p-values < 0.05: 31.667\n",
      "Fraction of FDR-corrected p-values < 0.05: 6.667\n",
      "blank -mp\n",
      "Fraction of uncorrected p-values < 0.05: 31.667\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.000\n",
      "blank -sp\n",
      "Fraction of uncorrected p-values < 0.05: 28.333\n",
      "Fraction of FDR-corrected p-values < 0.05: 0.000\n"
     ]
    }
   ],
   "source": [
    "store_perf = []\n",
    "\n",
    "perf = 'out_of_sample_r2'\n",
    "\n",
    "for dataset in ['pereira', 'fedorenko', 'blank']:\n",
    "    \n",
    "    for fe in ['', '-mp', '-sp']:\n",
    "    \n",
    "        all_gauss = []\n",
    "        \n",
    "        simple_perf = np.load(f'/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/figures_data/figure5/simple_combined_{dataset}.npz')['']\n",
    "        gpt2xlu_perf = np.load(f'/home2/ebrahim/beyond-brainscore/analyze_results/figures_code/figures_data/figure5/gpt2xl_combined_{dataset}.npz')[fe]\n",
    "        \n",
    "        perf_diff = gpt2xlu_perf - simple_perf\n",
    "        \n",
    "        for i in range(1000):\n",
    "            \n",
    "            if dataset == 'pereira':\n",
    "                \n",
    "                gauss_perf_combined = np.full(subjects_arr_pereira.shape[0], fill_value=np.nan)\n",
    "                \n",
    "                for e in exp:\n",
    "                \n",
    "                    gauss_perf = np.load(f'/data/LLMs/brainscore/results_{dataset}/stats/{dataset}_gaussian-stats_layer_{i}_1_{e}.npz')[perf]\n",
    "                    \n",
    "                    gauss_perf_combined[non_nan_indices_dict[e]] = custom_add_2d(gauss_perf_combined[non_nan_indices_dict[e]],  \n",
    "                                                                                    gauss_perf)\n",
    "                    \n",
    "                gauss_perf_combined = gauss_perf_combined[lang_indices]\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                gauss_perf_combined = np.load(f'/data/LLMs/brainscore/results_{dataset}/stats/{dataset}_gaussian-stats_layer_{i}_1.npz')[perf]\n",
    "                \n",
    "                \n",
    "            all_gauss.append(gauss_perf_combined)\n",
    "            \n",
    "        \n",
    "        all_gauss_np = np.stack(all_gauss)\n",
    "        print(dataset, fe)\n",
    "        _, _ = compute_voxel_pvalues(perf_diff, all_gauss_np)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
